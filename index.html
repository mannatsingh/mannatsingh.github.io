<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Mannat Singh - Research Engineer at Meta AI specializing in computer vision, self-supervised learning, and video generation">

    <!-- Open Graph / Social Media Meta Tags -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://mannatsingh.github.io/">
    <meta property="og:title" content="Mannat Singh | Research Engineer at Meta">
    <meta property="og:description" content="Research Engineer at Meta working on Generative AI, Computer Vision, Video Generation, and Self-Supervised Learning.">
    <meta property="og:image" content="https://mannatsingh.github.io/images/self.jpg">

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@mannat_singh">
    <meta name="twitter:title" content="Mannat Singh | Research Engineer at Meta">
    <meta name="twitter:description" content="Research Engineer at Meta working on Generative AI, Computer Vision, Video Generation, and Self-Supervised Learning.">
    <meta name="twitter:image" content="https://mannatsingh.github.io/images/self.jpg">

<title>Mannat Singh | Research Engineer</title>
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MCW4F26YZG"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-MCW4F26YZG');
    </script>
</head>
<body>
    <nav class="navbar">
        <div class="nav-content">
            <a href="#" class="nav-logo"></a>
<ul class="nav-links">
                <li><a href="#">Home</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#media">Media</a></li>
                <li><a href="#blogs">Blogs</a></li>
                <li><a href="#projects">Projects</a></li>
            </ul>
            <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
                <i class="fas fa-moon"></i>
            </button>
        </div>
    </nav>

    <header class="hero">
        <div class="hero-content">
            <div class="hero-text">
                <h1>Mannat Singh</h1>
                <p class="subtitle">Research Engineer at Meta</p>
<p class="bio">
                    I'm a Research Engineer at Meta working on Generative AI and Computer Vision.
                    My research focuses on video generation, multimodal learning, visual representation learning,
                    and self-supervised learning. I'm particularly interested in training large-scale models on
                    web-scale datasets through scalable and efficient approaches.
                </p>
<div class="social-links">
                    <a href="https://scholar.google.com/citations?user=QOO8OCcAAAAJ" target="_blank" title="Google Scholar">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                    <a href="https://github.com/mannatsingh" target="_blank" title="GitHub">
                        <i class="fab fa-github"></i>
                    </a>
                    <a href="https://twitter.com/mannat_singh" target="_blank" title="Twitter/X">
                        <i class="fab fa-twitter"></i>
                    </a>
                    <a href="https://www.linkedin.com/in/singh-mannat/" target="_blank" title="LinkedIn">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </div>
            </div>
<div class="hero-image">
                <div class="profile-placeholder">
                    <img src="images/self.jpg" alt="Mannat Singh">
                </div>
            </div>
        </div>
    </header>

    <main>
        <section id="about" class="section">
            <div class="container">
                <h2>Research Interests</h2>
<div class="research-areas">
                    <div class="research-card">
                        <i class="fas fa-video"></i>
                        <h3>Video Generation</h3>
                        <p>Text-to-video and video-editing generative models</p>
                    </div>
<div class="research-card">
                        <i class="fas fa-circle-nodes"></i>
                        <h3>Multimodal Learning</h3>
                        <p>Unified understanding of vision, language, and other modalities</p>
                    </div>
                    <div class="research-card">
                        <i class="fas fa-database"></i>
                        <h3>Large-Scale Learning</h3>
                        <p>Training and scaling models on billions of images and videos</p>
                    </div>
                    <div class="research-card">
                        <i class="fas fa-eye"></i>
                        <h3>Computer Vision</h3>
                        <p>Visual perception models, image classification, and object detection</p>
                    </div>
                    <div class="research-card">
                        <i class="fas fa-brain"></i>
                        <h3>Self-Supervised Learning</h3>
                        <p>Learning visual representations from unlabeled images and videos</p>
                    </div>

                </div>
            </div>
        </section>

        <section id="publications" class="section section-alt">
            <div class="container">
                <h2>Publications</h2>
                <p class="section-subtitle">
                    <a href="https://scholar.google.com/citations?user=QOO8OCcAAAAJ" target="_blank">
                        View all publications on Google Scholar <i class="fas fa-external-link-alt"></i>
                    </a>
                </p>

<div class="publications-list">
                    <!-- 2025 -->
                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/flowing-words-pixels.jpg" alt="Flowing from Words to Pixels" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution</h3>
                            <p class="authors">Q Liu, X Yin, A Yuille, A Brown, <strong>M Singh</strong></p>
                            <p class="venue">CVPR 2025 <span class="pub-badge highlight">Highlight</span></p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2412.15213" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://cross-flow.github.io/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                            </div>
                        </div>
                    </div>

                    <!-- 2024 -->
                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/moviegen.gif" alt="Movie Gen" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>Movie Gen: A Cast of Media Foundation Models</h3>
                            <p class="authors">A Polyak, A Zohar, A Brown, A Tjandra, ..., <strong>M Singh</strong>, ...</p>
                            <p class="venue">arXiv 2024</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2410.13720" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://ai.meta.com/research/movie-gen/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/llama3.gif" alt="Llama 3" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>The Llama 3 Herd of Models</h3>
                            <p class="authors">A Grattafiori, A Dubey, A Jauhri, A Pandey, ..., <strong>M Singh</strong>, ...</p>
                            <p class="venue">arXiv 2024</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2407.21783" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://llama.meta.com/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                            </div>
                        </div>
                    </div>

                    <!-- 2023 -->

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/emu-video.gif" alt="Emu Video" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning</h3>
                            <p class="authors">R Girdhar, <strong>M Singh</strong>, A Brown, Q Duval, S Azadi, SS Rambhatla, A Shah, ...</p>
                            <p class="venue">arXiv 2023</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2311.10709" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://emu-video.metademolab.com/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/imagebind.gif" alt="ImageBind" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>ImageBind: One Embedding Space to Bind Them All</h3>
                            <p class="authors">R Girdhar, A El-Nouby, Z Liu, <strong>M Singh</strong>, KV Alwala, A Joulin, I Misra</p>
<p class="venue">CVPR 2023 <span class="pub-badge highlight">Highlight</span></p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2305.05665" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://imagebind.metademolab.com/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                                <a href="https://github.com/facebookresearch/ImageBind" target="_blank"><i class="fab fa-github"></i> Code</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/mae-prepretraining.png" alt="MAE Pre-Pretraining" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining</h3>
                            <p class="authors"><strong>M Singh</strong>, Q Duval, KV Alwala, H Fan, V Aggarwal, A Adcock, A Joulin, ...</p>
                            <p class="venue">ICCV 2023</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2303.13496" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/omnimae.png" alt="OmniMAE" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>OmniMAE: Single Model Masked Pretraining on Images and Videos</h3>
                            <p class="authors">R Girdhar, A El-Nouby, <strong>M Singh</strong>, KV Alwala, A Joulin, I Misra</p>
                            <p class="venue">CVPR 2023</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2206.08356" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://github.com/facebookresearch/omnivore" target="_blank"><i class="fab fa-github"></i> Code</a>
                            </div>
                        </div>
                    </div>

                    <!-- 2022 -->
<div class="publication">
                        <div class="pub-image">
                            <img src="images/omnivore.jpg" alt="Omnivore" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>Omnivore: A Single Model for Many Visual Modalities</h3>
                            <p class="authors">R Girdhar, <strong>M Singh</strong>, N Ravi, L van der Maaten, A Joulin, I Misra</p>
                            <p class="venue">CVPR 2022 <span class="pub-badge oral">Oral</span></p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2201.08377" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://github.com/facebookresearch/omnivore" target="_blank"><i class="fab fa-github"></i> Code</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/weakly-supervised.png" alt="Weakly Supervised Pre-Training" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>Revisiting Weakly Supervised Pre-Training of Visual Perception Models</h3>
                            <p class="authors"><strong>M Singh</strong>, L Gustafson, A Adcock, VF Reis, B Gedik, RP Kosaraju, ...</p>
                            <p class="venue">CVPR 2022</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2201.08371" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                    <!-- 2021 -->
                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/vitconv.png" alt="Early Convolutions" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>Early Convolutions Help Transformers See Better</h3>
                            <p class="authors">T Xiao, <strong>M Singh</strong>, E Mintun, T Darrell, P Dollár, R Girshick</p>
                            <p class="venue">NeurIPS 2021</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2106.14881" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/mdetr.png" alt="MDETR" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>MDETR - Modulated Detection for End-to-End Multi-Modal Understanding</h3>
                            <p class="authors">A Kamath, <strong>M Singh</strong>, Y LeCun, G Synnaeve, I Misra, N Carion</p>
                            <p class="venue">ICCV 2021 <span class="pub-badge oral">Oral</span></p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2104.12763" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://github.com/ashkamath/mdetr" target="_blank"><i class="fab fa-github"></i> Code</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/fast-model-scaling.png" alt="Fast and Accurate Model Scaling" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>Fast and Accurate Model Scaling</h3>
                            <p class="authors">P Dollár, <strong>M Singh</strong>, R Girshick</p>
                            <p class="venue">CVPR 2021</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2103.06877" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/seer.png" alt="SEER Self-supervised Pretraining" loading="lazy">
                        </div>
                        <div class="pub-content">
                            <h3>Self-supervised Pretraining of Visual Features in the Wild</h3>
                            <p class="authors">P Goyal, M Caron, B Lefaudeux, M Xu, P Wang, V Pai, <strong>M Singh</strong>, ...</p>
                            <p class="venue">arXiv 2021</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2103.01988" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                </div>

<h3 id="media" class="subsection-title">Media Coverage</h3>
                <div class="news-list">
                    <a href="https://www.theverge.com/2024/10/4/24261990/meta-movie-gen-ai-video-generator-openai-sora" target="_blank" class="news-item">
                        <span class="news-source"><img src="https://www.google.com/s2/favicons?domain=theverge.com&sz=32" alt=""> The Verge</span>
                        <span class="news-date">Oct 2024</span>
                        <span class="news-title">Meta announces Movie Gen, an AI-powered video generator</span>
                    </a>
                    <a href="https://techcrunch.com/2024/10/04/metas-movie-gen-model-puts-out-realistic-video-with-sound-so-we-can-finally-have-infinite-moo-deng/" target="_blank" class="news-item">
                        <span class="news-source"><img src="https://www.google.com/s2/favicons?domain=techcrunch.com&sz=32" alt=""> TechCrunch</span>
                        <span class="news-date">Oct 2024</span>
                        <span class="news-title">Meta's Movie Gen model puts out realistic video with sound, so we can finally have infinite Moo Deng</span>
                    </a>
                    <a href="https://www.wired.com/story/metas-open-source-llama-3-nipping-at-openais-heels/" target="_blank" class="news-item">
                        <span class="news-source"><img src="https://www.google.com/s2/favicons?domain=wired.com&sz=32" alt=""> Wired</span>
                        <span class="news-date">Apr 2024</span>
                        <span class="news-title">Meta's Open Source Llama 3 Is Already Nipping at OpenAI's Heels</span>
                    </a>
                    <a href="https://techcrunch.com/2023/11/16/meta-brings-us-a-step-closer-to-ai-generated-movies/" target="_blank" class="news-item">
                        <span class="news-source"><img src="https://www.google.com/s2/favicons?domain=techcrunch.com&sz=32" alt=""> TechCrunch</span>
                        <span class="news-date">Nov 2023</span>
                        <span class="news-title">Meta brings us a step closer to AI-generated movies</span>
                    </a>
                    <a href="https://www.theverge.com/2023/5/9/23716558/meta-imagebind-open-source-multisensory-modal-ai-model-research" target="_blank" class="news-item">
                        <span class="news-source"><img src="https://www.google.com/s2/favicons?domain=theverge.com&sz=32" alt=""> The Verge</span>
                        <span class="news-date">May 2023</span>
                        <span class="news-title">Meta open-sources multisensory AI model that combines six types of data</span>
                    </a>
                    <a href="https://www.wired.com/story/facebook-new-ai-teaches-itself-see-less-human-help/" target="_blank" class="news-item">
                        <span class="news-source"><img src="https://www.google.com/s2/favicons?domain=wired.com&sz=32" alt=""> Wired</span>
                        <span class="news-date">Mar 2021</span>
                        <span class="news-title">Facebook's New AI Teaches Itself to See With Less Human Help</span>
                    </a>
                </div>

                <h3 id="blogs" class="subsection-title">Blogs</h3>
                <div class="news-list">
                    <a href="https://ai.meta.com/blog/movie-gen-media-foundation-models-generative-ai-video/" target="_blank" class="news-item">
                        <span class="news-source">Meta AI Blog</span>
                        <span class="news-date">Oct 2024</span>
                        <span class="news-title">How Meta Movie Gen could usher in a new AI-enabled era for content creators</span>
                    </a>
                    <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" target="_blank" class="news-item">
                        <span class="news-source">Meta AI Blog</span>
                        <span class="news-date">Sep 2024</span>
                        <span class="news-title">Llama 3.2: Revolutionizing edge AI and vision with open, customizable models</span>
                    </a>
                    <a href="https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/" target="_blank" class="news-item">
                        <span class="news-source">Meta AI Blog</span>
                        <span class="news-date">Nov 2023</span>
                        <span class="news-title">Emu Video and Emu Edit: Our latest generative AI research milestones</span>
                    </a>
                    <a href="https://ai.meta.com/blog/imagebind-six-modalities-binding-ai/" target="_blank" class="news-item">
                        <span class="news-source">Meta AI Blog</span>
                        <span class="news-date">May 2023</span>
                        <span class="news-title">ImageBind: Holistic AI learning across six modalities</span>
                    </a>
                    <a href="https://ai.meta.com/blog/seer-10b-better-fairer-computer-vision-through-self-supervised-learning-training-on-diverse-datasets/" target="_blank" class="news-item">
                        <span class="news-source">Meta AI Blog</span>
                        <span class="news-date">Mar 2021</span>
                        <span class="news-title">SEER 10B: Better, fairer computer vision through self-supervised learning on diverse datasets</span>
                    </a>
                </div>

                <h3 id="projects" class="subsection-title">Open Source Projects</h3>
                <div class="projects-grid">
                    <a href="https://github.com/facebookresearch/vissl" target="_blank" class="project-card">
                        <h4>VISSL</h4>
                        <p>A library for state-of-the-art Self-Supervised Learning with PyTorch</p>
                        <span class="project-meta"><i class="fab fa-github"></i> facebookresearch/vissl</span>
                        <img class="github-stars" src="https://img.shields.io/github/stars/facebookresearch/vissl?style=social" alt="GitHub stars">
                    </a>
                    <a href="https://github.com/facebookresearch/pycls" target="_blank" class="project-card">
                        <h4>pycls</h4>
                        <p>A flexible codebase for image classification with RegNet and model scaling</p>
                        <span class="project-meta"><i class="fab fa-github"></i> facebookresearch/pycls</span>
                        <img class="github-stars" src="https://img.shields.io/github/stars/facebookresearch/pycls?style=social" alt="GitHub stars">
                    </a>
                    <a href="https://github.com/facebookresearch/ClassyVision" target="_blank" class="project-card">
                        <h4>Classy Vision</h4>
                        <p>An end-to-end framework for image and video classification</p>
                        <span class="project-meta"><i class="fab fa-github"></i> facebookresearch/ClassyVision</span>
                        <img class="github-stars" src="https://img.shields.io/github/stars/facebookresearch/ClassyVision?style=social" alt="GitHub stars">
                    </a>
                    <a href="https://github.com/facebookresearch/NeuralCompression" target="_blank" class="project-card">
                        <h4>NeuralCompression</h4>
                        <p>A collection of neural compression tools and methods</p>
                        <span class="project-meta"><i class="fab fa-github"></i> facebookresearch/NeuralCompression</span>
                        <img class="github-stars" src="https://img.shields.io/github/stars/facebookresearch/NeuralCompression?style=social" alt="GitHub stars">
                    </a>
                </div>
            </div>
        </section>

    </main>

<footer>
        <div class="container">
            <p>&copy; 2025 Mannat Singh. All rights reserved.</p>
        </div>
    </footer>

    <!-- Back to Top Button -->
    <button class="back-to-top" id="backToTop" aria-label="Back to top">
        <i class="fas fa-arrow-up"></i>
    </button>

    <script>
        // Dark mode toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');

        // Check for saved theme preference or default to dark
        const savedTheme = localStorage.getItem('theme') || 'dark';
        html.setAttribute('data-theme', savedTheme);
        updateIcon(savedTheme);

        themeToggle.addEventListener('click', () => {
            const currentTheme = html.getAttribute('data-theme');
            const newTheme = currentTheme === 'light' ? 'dark' : 'light';

            html.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
            updateIcon(newTheme);
        });

        function updateIcon(theme) {
            icon.className = theme === 'light' ? 'fas fa-moon' : 'fas fa-sun';
        }

        // Back to Top Button
        const backToTop = document.getElementById('backToTop');

        window.addEventListener('scroll', () => {
            if (window.scrollY > 300) {
                backToTop.classList.add('visible');
            } else {
                backToTop.classList.remove('visible');
            }
        });

        backToTop.addEventListener('click', () => {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Active Nav Highlighting
        const sections = document.querySelectorAll('section[id], h3[id]');
        const navLinks = document.querySelectorAll('.nav-links a');

        function updateActiveNav() {
            let current = '';
            const scrollPos = window.scrollY + 100;

            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                if (scrollPos >= sectionTop) {
                    current = section.getAttribute('id');
                }
            });

            navLinks.forEach(link => {
                link.classList.remove('active');
                const href = link.getAttribute('href');
                if (href === '#' && window.scrollY < 200) {
                    link.classList.add('active');
                } else if (href === '#' + current) {
                    link.classList.add('active');
                } else if (current === 'about' && href === '#') {
                    link.classList.add('active');
                }
            });
        }

        window.addEventListener('scroll', updateActiveNav);
        updateActiveNav();

        // Scroll Animations
        const fadeElements = document.querySelectorAll('.publication, .research-card, .project-card, .news-item');

        fadeElements.forEach(el => el.classList.add('fade-in'));

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.classList.add('visible');
                }
            });
        }, { threshold: 0.1, rootMargin: '0px 0px -50px 0px' });

        fadeElements.forEach(el => observer.observe(el));
    </script>
</body>
</html>
