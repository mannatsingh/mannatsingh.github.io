<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Mannat Singh - Research Scientist at Meta AI specializing in computer vision, self-supervised learning, and video generation">
    <title>Mannat Singh | Research Scientist</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <nav class="navbar">
        <div class="nav-content">
            <a href="#" class="nav-logo">Mannat Singh</a>
            <ul class="nav-links">
                <li><a href="#about">About</a></li>
                <li><a href="#publications">Publications</a></li>
            </ul>
        </div>
    </nav>

    <header class="hero">
        <div class="hero-content">
            <div class="hero-text">
                <h1>Mannat Singh</h1>
                <p class="subtitle">Research Scientist at Meta</p>
                <p class="bio">
                    I'm a Research Scientist at Meta working on computer vision and generative AI.
                    My research focuses on self-supervised learning, visual representation learning,
                    and video generation. I'm particularly interested in developing scalable methods
                    for learning from large-scale visual data.
                </p>
<div class="social-links">
                    <a href="https://scholar.google.com/citations?user=QOO8OCcAAAAJ" target="_blank" title="Google Scholar">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                    <a href="https://github.com/mannatsingh" target="_blank" title="GitHub">
                        <i class="fab fa-github"></i>
                    </a>
                    <a href="https://twitter.com/manaborex" target="_blank" title="Twitter/X">
                        <i class="fab fa-twitter"></i>
                    </a>
                    <a href="https://www.linkedin.com/in/mannat-singh" target="_blank" title="LinkedIn">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </div>
            </div>
            <div class="hero-image">
                <div class="profile-placeholder">
                    <i class="fas fa-user"></i>
                </div>
            </div>
        </div>
    </header>

    <main>
        <section id="about" class="section">
            <div class="container">
                <h2>Research Interests</h2>
                <div class="research-areas">
                    <div class="research-card">
                        <i class="fas fa-eye"></i>
                        <h3>Computer Vision</h3>
                        <p>Visual perception models, image classification, and object detection at scale</p>
                    </div>
                    <div class="research-card">
                        <i class="fas fa-brain"></i>
                        <h3>Self-Supervised Learning</h3>
                        <p>Learning visual representations from unlabeled images and videos</p>
                    </div>
                    <div class="research-card">
                        <i class="fas fa-video"></i>
                        <h3>Video Generation</h3>
                        <p>Text-to-video generation and multimodal generative models</p>
                    </div>
                    <div class="research-card">
                        <i class="fas fa-database"></i>
                        <h3>Large-Scale Learning</h3>
                        <p>Training and scaling models on billions of images and videos</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="publications" class="section section-alt">
            <div class="container">
                <h2>Selected Publications</h2>
                <p class="section-subtitle">
                    <a href="https://scholar.google.com/citations?user=QOO8OCcAAAAJ" target="_blank">
                        View all publications on Google Scholar <i class="fas fa-external-link-alt"></i>
                    </a>
                </p>

<div class="publications-list">
                    <!-- 2025 -->
                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/flowing-words-pixels.jpg" alt="Flowing from Words to Pixels">
                        </div>
                        <div class="pub-content">
                            <h3>Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution</h3>
                            <p class="authors">Q Liu, X Yin, A Yuille, A Brown, <strong>M Singh</strong></p>
                            <p class="venue">CVPR 2025</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2412.15213" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://word2pix.github.io/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                            </div>
                        </div>
                    </div>

                    <!-- 2024 -->
                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/llama3.gif" alt="Llama 3">
                        </div>
                        <div class="pub-content">
                            <h3>The Llama 3 Herd of Models</h3>
                            <p class="authors">A Grattafiori, A Dubey, A Jauhri, A Pandey, ..., <strong>M Singh</strong>, ...</p>
                            <p class="venue">arXiv 2024</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2407.21783" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://llama.meta.com/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/moviegen.gif" alt="Movie Gen">
                        </div>
                        <div class="pub-content">
                            <h3>Movie Gen: A Cast of Media Foundation Models</h3>
                            <p class="authors">A Polyak, A Zohar, A Brown, A Tjandra, ..., <strong>M Singh</strong>, ...</p>
                            <p class="venue">arXiv 2024</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2410.13720" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://ai.meta.com/research/movie-gen/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                            </div>
                        </div>
                    </div>

                    <!-- 2023 -->
                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/imagebind.gif" alt="ImageBind">
                        </div>
                        <div class="pub-content">
                            <h3>ImageBind: One Embedding Space to Bind Them All</h3>
                            <p class="authors">R Girdhar, A El-Nouby, Z Liu, <strong>M Singh</strong>, KV Alwala, A Joulin, I Misra</p>
                            <p class="venue">CVPR 2023</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2305.05665" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://imagebind.metademolab.com/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                                <a href="https://github.com/facebookresearch/ImageBind" target="_blank"><i class="fab fa-github"></i> Code</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/emu-video.gif" alt="Emu Video">
                        </div>
                        <div class="pub-content">
                            <h3>Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning</h3>
                            <p class="authors">R Girdhar, <strong>M Singh</strong>, A Brown, Q Duval, S Azadi, SS Rambhatla, A Shah, ...</p>
                            <p class="venue">arXiv 2023</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2311.10709" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://emu-video.metademolab.com/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/mae-prepretraining.png" alt="MAE Pre-Pretraining">
                        </div>
                        <div class="pub-content">
                            <h3>The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining</h3>
                            <p class="authors"><strong>M Singh</strong>, Q Duval, KV Alwala, H Fan, V Aggarwal, A Adcock, A Joulin, ...</p>
                            <p class="venue">ICCV 2023</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2303.13496" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/omnimae.png" alt="OmniMAE">
                        </div>
                        <div class="pub-content">
                            <h3>OmniMAE: Single Model Masked Pretraining on Images and Videos</h3>
                            <p class="authors">R Girdhar, A El-Nouby, <strong>M Singh</strong>, KV Alwala, A Joulin, I Misra</p>
                            <p class="venue">CVPR 2023</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2206.08356" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://github.com/facebookresearch/omnivore" target="_blank"><i class="fab fa-github"></i> Code</a>
                            </div>
                        </div>
                    </div>

                    <!-- 2022 -->
                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/omnivore.jpg" alt="Omnivore">
                        </div>
                        <div class="pub-content">
                            <h3>Omnivore: A Single Model for Many Visual Modalities</h3>
                            <p class="authors">R Girdhar, <strong>M Singh</strong>, N Ravi, L van der Maaten, A Joulin, I Misra</p>
                            <p class="venue">CVPR 2022</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2201.08377" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://github.com/facebookresearch/omnivore" target="_blank"><i class="fab fa-github"></i> Code</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/weakly-supervised.png" alt="Weakly Supervised Pre-Training">
                        </div>
                        <div class="pub-content">
                            <h3>Revisiting Weakly Supervised Pre-Training of Visual Perception Models</h3>
                            <p class="authors"><strong>M Singh</strong>, L Gustafson, A Adcock, VF Reis, B Gedik, RP Kosaraju, ...</p>
                            <p class="venue">CVPR 2022</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2201.08371" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                    <!-- 2021 -->
                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/mdetr.png" alt="MDETR">
                        </div>
                        <div class="pub-content">
                            <h3>MDETR - Modulated Detection for End-to-End Multi-Modal Understanding</h3>
                            <p class="authors">A Kamath, <strong>M Singh</strong>, Y LeCun, G Synnaeve, I Misra, N Carion</p>
                            <p class="venue">ICCV 2021</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2104.12763" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                                <a href="https://github.com/ashkamath/mdetr" target="_blank"><i class="fab fa-github"></i> Code</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/vitconv.png" alt="Early Convolutions">
                        </div>
                        <div class="pub-content">
                            <h3>Early Convolutions Help Transformers See Better</h3>
                            <p class="authors">T Xiao, <strong>M Singh</strong>, E Mintun, T Darrell, P Dollár, R Girshick</p>
                            <p class="venue">NeurIPS 2021</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2106.14881" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/seer.png" alt="SEER Self-supervised Pretraining">
                        </div>
                        <div class="pub-content">
                            <h3>Self-supervised Pretraining of Visual Features in the Wild</h3>
                            <p class="authors">P Goyal, M Caron, B Lefaudeux, M Xu, P Wang, V Pai, <strong>M Singh</strong>, ...</p>
                            <p class="venue">arXiv 2021</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2103.01988" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>

                    <div class="publication">
                        <div class="pub-image">
                            <img src="images/fast-model-scaling.png" alt="Fast and Accurate Model Scaling">
                        </div>
                        <div class="pub-content">
                            <h3>Fast and Accurate Model Scaling</h3>
                            <p class="authors">P Dollár, <strong>M Singh</strong>, R Girshick</p>
                            <p class="venue">CVPR 2021</p>
                            <div class="pub-links">
                                <a href="https://arxiv.org/abs/2103.06877" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                            </div>
                        </div>
                    </div>
                </div>

                <h3 class="subsection-title">Open Source Projects</h3>
                <div class="projects-grid">
                    <a href="https://github.com/facebookresearch/vissl" target="_blank" class="project-card">
                        <h4>VISSL</h4>
                        <p>A library for state-of-the-art Self-Supervised Learning with PyTorch</p>
                        <span class="project-meta"><i class="fab fa-github"></i> facebookresearch/vissl</span>
                    </a>
                    <a href="https://github.com/facebookresearch/ClassyVision" target="_blank" class="project-card">
                        <h4>Classy Vision</h4>
                        <p>An end-to-end framework for image and video classification</p>
                        <span class="project-meta"><i class="fab fa-github"></i> facebookresearch/ClassyVision</span>
                    </a>
                    <a href="https://github.com/facebookresearch/NeuralCompression" target="_blank" class="project-card">
                        <h4>NeuralCompression</h4>
                        <p>A collection of neural compression tools and methods</p>
                        <span class="project-meta"><i class="fab fa-github"></i> facebookresearch/NeuralCompression</span>
                    </a>
                </div>
            </div>
        </section>

    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 Mannat Singh. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
